<!DOCTYPE html>
<html lang="en-us">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="generator" content="Source Themes Academic 4.5.0">


    <meta name="author" content="Yunzhi Lin">


    <meta name="description" content="Ph.D. Candidate">


    <link rel="alternate" hreflang="en-us" href="https://yunzhi.netlify.com/">


    <meta name="theme-color" content="#3f51b5">


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css"
          integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css"
          integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css"
          integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css"
          crossorigin="anonymous" title="hl-light">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css"
          crossorigin="anonymous" title="hl-dark" disabled>


    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">


    <link rel="stylesheet" href="/css/academic.min.0523168da2f97c9b4381021bbaf8b1c7.css">


    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Yunzhi Lin">


    <link rel="manifest" href="/index.webmanifest">
    <link rel="icon" type="image/png" href="/img/icon-32.png">
    <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

    <link rel="canonical" href="https://yunzhi.netlify.com/">


    <meta property="twitter:card" content="summary">

    <meta property="og:site_name" content="Yunzhi Lin">
    <meta property="og:url" content="https://yunzhi.netlify.com/">
    <meta property="og:title" content="Yunzhi Lin">
    <meta property="og:description" content="Ph.D. Candidate">
    <meta property="og:image" content="https://yunzhi.netlify.com/img/icon-192.png">
    <meta property="twitter:image" content="https://yunzhi.netlify.com/img/icon-192.png">
    <meta property="og:locale" content="en-us">

    <meta property="og:updated_time" content="2023-01-20T00:00:00&#43;00:00">


    <script type="application/ld+json">
        {
            "@context": "https://schema.org",
            "@type": "WebSite",
            "potentialAction": {
                "@type": "SearchAction",
                "target": "https://yunzhi.netlify.com/?q={search_term_string}",
                "query-input": "required name=search_term_string"
            },
            "url": "https://yunzhi.netlify.com/"
        }
    </script>


    <title>Yunzhi Lin</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main">

<aside class="search-results" id="search">
    <div class="container">
        <section class="search-header">

            <div class="row no-gutters justify-content-between mb-3">
                <div class="col-6">
                    <h1>Search</h1>
                </div>
                <div class="col-6 col-search-close">
                    <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
                </div>
            </div>

            <div id="search-box">

                <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
                       autocomplete="off" autocorrect="off" spellcheck="false" type="search">

            </div>

        </section>
        <section class="section-search-results">

            <div id="search-hits">

            </div>

        </section>
    </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
    <div class="container">


        <a class="navbar-brand" href="/">Yunzhi Lin</a>

        <button type="button" class="navbar-toggler" data-toggle="collapse"
                data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span><i class="fas fa-bars"></i></span>
        </button>


        <div class="collapse navbar-collapse" id="navbar">


            <ul class="navbar-nav mr-auto">


                <li class="nav-item">
                    <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
                </li>


                <li class="nav-item">
                    <a class="nav-link " href="/#publications" data-target="#publications"><span>Publications</span></a>
                </li>


                <li class="nav-item">
                    <a class="nav-link " href="/pdf/CV_Yunzhi_Lin.pdf"><span>Resume</span></a>
                </li>


                <li class="nav-item">
                    <a class="nav-link " href="/#contact" data-target="#contact"><span>Contact</span></a>
                </li>


            </ul>
            <ul class="navbar-nav ml-auto">


                <li class="nav-item">
                    <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
                </li>


                <li class="nav-item">
                    <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
                </li>


            </ul>

        </div>
    </div>
</nav>


<span class="js-widget-page d-none"></span>


<section id="about" class="home-section wg-about   ">
    <div class="container">


        <div class="row">
            <div class="col-12 col-lg-4">
                <div id="profile">


                    <img class="portrait"
                         src="/authors/admin/avatar_hub52ee1ce05e72a11204e6b46c8d771a9_484901_250x250_fill_q90_lanczos_center.jpg"
                         alt="Avatar">


                    <div class="portrait-title">
                        <h2>Yunzhi Lin</h2>
                        <h3>Ph.D. Candidate</h3>


                        <h3>
                            <a href="http://ivalab.gatech.edu" target="_blank" rel="noopener">
                                <span>Intelligent Vision and Automation Laboratory</span>
                            </a>
                        </h3>

                        <h3>
                            <a href="http://robotics.gatech.edu/" target="_blank" rel="noopener">
                                <span>IRIM, Georgia Institute of Technology</span>
                            </a>
                        </h3>

                    </div>

                    <ul class="network-icon" aria-hidden="true">


                        <li>
                            <a href="/#contact">
                                <i class="fas fa-envelope big-icon"></i>
                            </a>
                        </li>


                        <li>
                            <a href="https://twitter.com/lyz14115" target="_blank" rel="noopener">
                                <i class="fab fa-twitter big-icon"></i>
                            </a>
                        </li>


                        <li>
                            <a href="https://scholar.google.com/citations?user=tPv9f7YAAAAJ&amp;hl=en" target="_blank"
                               rel="noopener">
                                <i class="ai ai-google-scholar big-icon"></i>
                            </a>
                        </li>


                        <li>
                            <a href="https://www.linkedin.com/in/yunzhi-lin-53165a139/" target="_blank" rel="noopener">
                                <i class="fab fa-linkedin big-icon"></i>
                            </a>
                        </li>

                    </ul>

                </div>
            </div>
            <div class="col-12 col-lg-8">


                <h1>About</h1>

                <p>I am an ECE Ph.D. candidate advised by <span style="color:blue"><a
                        href="http://ivalab.gatech.edu/people/pvela.html" target="_blank">Patricio A. Vela</a></span> at
                    Institute for Robotics and Intelligent Machines, Georgia Tech. I also collaborate closely with <a
                            href="https://research.nvidia.com/person/jonathan-tremblay" target="_blank">Jonathan
                        Tremblay</a>, <a href="https://research.nvidia.com/person/stephen-tyree" target="_blank">Stephen
                        Tyree</a>, <a href="https://research.nvidia.com/person/bowen-wen" target="_blank">Bowen Wen</a>,
                    and <a href="https://research.nvidia.com/person/stan-birchfield" target="_blank">Stan Birchfield</a>
                    from NVIDIA Research. I am a practical roboticist, interested in combining robotics and computer
                    vision together to solve the challenges in the real world. To be more specific, I am working on
                    6-DoF Object Pose Estimation, Object Grasping, and Human-robot Interaction related topics. <a
                            href="/pdf/CV_Yunzhi_Lin.pdf"><span style="color:blue">[View my CV]</span></a> (Updated in
                    Jan. 2023).</p>

                <p>I received B.E. in Automation from Southeast University in 2018, advised by <span style="color:blue"><a
                        href="https://scholar.google.com/citations?hl=zh-CN&amp;user=0iHboRcAAAAJ&amp;view_op=list_works"
                        target="_blank">Wenze Shao</a></span> and <span style="color:blue"><a
                        href="https://www.yangangwang.com/" target="_blank">Yangang Wang</a></span>. I was also a
                    research intern at Applied Nonlinear Control Lab, University of Alberta, in 2017, advised by <span
                            style="color:blue"><a href="https://apps.ualberta.ca/directory/person/alynch"
                                                  target="_blank">Alan Lynch</a></span>; <a
                            href="https://nvlabs.github.io/lpr/" target="_blank">NVIDIA Learning and Perception Research
                        group</a> from May 2020 to May 2021 and May 2022 to Dec 2022, advised by <a
                            href="https://research.nvidia.com/person/stan-birchfield" target="_blank">Stan
                        Birchfield</a>.</p>


                <h1>News</h1>

                <div class="row">

                    <div class="col-md-20">
                        <ul class="ul-news">

                            <li><strong>&#91;Jan 2023&#93;</strong> Two papers accepted to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;ICRA 2023&nbsp;</strong></span>
                                about our work <a href="https://arxiv.org/abs/2210.10108" target="_blank">Parallel NeRF
                                    for Pose Estimation</a> and <a href="https://arxiv.org/abs/2209.08752"
                                                                   target="_blank">Keypoint-GraspNet</a></li>

                            <li><strong>&#91;June 2022&#93;</strong> One paper accepted to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;RA-L with IROS 2022 option&nbsp;</strong></span>
                                about our work <a href="https://arxiv.org/abs/2202.12912" target="_blank">Symbolic Goal
                                    Learning in a Hybrid, Modular Framework for Human Instruction Following</a></li>

                            <li><strong>&#91;May 2022&#93;</strong> Great to come back to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;NVIDIA&nbsp;</strong></span>
                                Learning and Perception Research Group to work as a research intern
                            </li>

                            <li><strong>&#91;Jan 2022&#93;</strong> Two papers accepted to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;ICRA 2022&nbsp;</strong></span>
                                about our work <a href="https://arxiv.org/abs/2109.06161" target="_blank">CenterPose</a>
                                and <a href="https://arxiv.org/abs/2205.11047" target="_blank">CenterPoseTrack</a></li>

                            <li><strong>&#91;June 2021&#93;</strong> One paper accepted to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;IROS 2021&nbsp;</strong></span>
                                about our work <a href="https://arxiv.org/abs/2103.13539" target="_blank">Multi-view
                                    Fusion for Multi-level Robotic Scene Understanding</a></li>

                            <li><strong>&#91;Jan 2021&#93;</strong> One paper accepted to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;ICRA 2021&nbsp;</strong></span>
                                about our work <a
                                        href="../../../pdf/A Joint Network for Grasp Detection Conditioned on Natural Language Commands.pdf">A
                                    Joint Network for Grasp Detection Conditioned on Natural Language Commands</a></li>

                            <li><strong>&#91;May 2020&#93;</strong> Excited to join <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;NVIDIA&nbsp;</strong></span>
                                Learning and Perception Research Group to work as a research intern
                            </li>

                            <li><strong>&#91;Jan 2020&#93;</strong> One paper accepted to <span
                                    style="background-color:LavenderBlush;color:MediumVioletRed"><strong>&nbsp;ICRA 2020&nbsp;</strong></span>
                                about our work <a
                                        href="../../../pdf/Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping.pdf">Using
                                    Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object
                                    Grasping</a></li>

                        </ul>
                    </div>

                </div>

                <div class="row">


                    <div class="col-md-5">
                        <h3>Interests</h3>
                        <ul class="ul-interests">

                            <li>Robotics</li>

                            <li>Computer Vision</li>

                            <li>Image Processing</li>

                        </ul>
                    </div>


                    <div class="col-md-7">
                        <h3>Education</h3>
                        <ul class="ul-edu fa-ul">

                            <li>
                                <i class="fa-li fas fa-graduation-cap"></i>
                                <div class="description">
                                    <p class="course">Ph.D. in Electrical and Computer Engineering, 2023 (Expected)</p>
                                    <p class="institution">Georgia Institute of Technology</p>
                                </div>
                            </li>

                            <li>
                                <i class="fa-li fas fa-graduation-cap"></i>
                                <div class="description">
                                    <p class="course">M.S. in Electrical and Computer Engineering, 2020</p>
                                    <p class="institution">Georgia Institute of Technology</p>
                                </div>
                            </li>

                            <li>
                                <i class="fa-li fas fa-graduation-cap"></i>
                                <div class="description">
                                    <p class="course">B.E. in Automation, 2018</p>
                                    <p class="institution">Southeast University</p>
                                </div>
                            </li>

                        </ul>
                    </div>


                </div>
            </div>
        </div>

    </div>
</section>


<section id="featured" class="home-section wg-featured   ">
    <div class="container">


        <div class="row">
            <div class="col-12 col-lg-4 section-heading">
                <h1>Featured Publications</h1>

            </div>
            <div class="col-12 col-lg-8">


                <div class="media stream-item">
                    <div class="media-body">

                        <h3 class="article-title mb-0 mt-0">
                            <a href="/publication/inerfparallel/">Parallel Inversion of Neural Radiance Fields for
                                Robust Pose Estimation</a>
                        </h3>


                        <div class="article-style">


                            <p>A parallelized optimization method based on fast Neural Radiance Fields (NeRF) for
                                estimating 6-DoF target poses.</p>
                        </div>


                        <div class="stream-meta article-metadata">


                            <div>


                                <span><strong>Yunzhi Lin</strong></span>, <span>Thomas Müller</span>, <span>Jonathan Tremblay</span>,
                                <span>Bowen Wen</span>, <span>Stephen Tyree</span>, <span>Alex Evans</span>, <span>Patricio A. Vela</span>,
                                <span>Stan Birchfield</span>

                            </div>

                        </div>


                        <div class="article-metadata-publication-short">ICRA 2023</div>


                        <div class="btn-links">


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2210.10108"
                               target="_blank" rel="noopener">
                                Preprint
                            </a>


                            <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                    data-filename="/publication/inerfparallel/cite.bib">
                                Cite
                            </button>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://pnerfp.github.io/"
                               target="_blank" rel="noopener">
                                Project
                            </a>


                        </div>


                    </div>
                    <div class="ml-3">


                        <a href="/publication/inerfparallel/">
                            <img src="/publication/inerfparallel/icon_hu69de85be5b7937724118f7aa440a7a23_61354_200x0_resize_lanczos_2.png"
                                 alt="">
                        </a>

                    </div>
                </div>


                <div class="media stream-item">
                    <div class="media-body">

                        <h3 class="article-title mb-0 mt-0">
                            <a href="/publication/centerposetrack/">Keypoint-Based Category-Level Object Pose Tracking
                                from an RGB Sequence with Uncertainty Estimation</a>
                        </h3>


                        <div class="article-style">


                            <p>A single-stage, category-level 6-DoF pose estimation algorithm that simultaneously
                                detects and tracks instances of objects within a known category.</p>
                        </div>


                        <div class="stream-meta article-metadata">


                            <div>


                                <span><strong>Yunzhi Lin</strong></span>, <span>Jonathan Tremblay</span>, <span>Stephen Tyree</span>,
                                <span>Patricio A. Vela</span>, <span>Stan Birchfield</span>

                            </div>

                        </div>


                        <div class="article-metadata-publication-short">ICRA 2022</div>


                        <div class="btn-links">


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2205.11047"
                               target="_blank" rel="noopener">
                                Preprint
                            </a>


                            <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                    data-filename="/publication/centerposetrack/cite.bib">
                                Cite
                            </button>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://github.com/NVlabs/CenterPose" target="_blank" rel="noopener">
                                Code
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://sites.google.com/view/centerposetrack" target="_blank" rel="noopener">
                                Project
                            </a>


                        </div>


                    </div>
                    <div class="ml-3">


                        <a href="/publication/centerposetrack/">
                            <img src="/publication/centerposetrack/icon_hu710a486516885f7d7b64c5d3e33ff1f6_43319_200x0_resize_lanczos_2.png"
                                 alt="">
                        </a>

                    </div>
                </div>


                <div class="media stream-item">
                    <div class="media-body">

                        <h3 class="article-title mb-0 mt-0">
                            <a href="/publication/centerpose/">Single-Stage Keypoint-based Category-level Object Pose
                                Estimation from an RGB Image</a>
                        </h3>


                        <div class="article-style">


                            <p>A single-stage, keypoint-based approach for category-level object pose estimation that
                                operates on unknown object instances within a known category using a single RGB image as
                                input.</p>
                        </div>


                        <div class="stream-meta article-metadata">


                            <div>


                                <span><strong>Yunzhi Lin</strong></span>, <span>Jonathan Tremblay</span>, <span>Stephen Tyree</span>,
                                <span>Patricio A. Vela</span>, <span>Stan Birchfield</span>

                            </div>

                        </div>


                        <div class="article-metadata-publication-short">ICRA 2022</div>


                        <div class="btn-links">


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2109.06161"
                               target="_blank" rel="noopener">
                                Preprint
                            </a>


                            <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                    data-filename="/publication/centerpose/cite.bib">
                                Cite
                            </button>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://github.com/NVlabs/CenterPose" target="_blank" rel="noopener">
                                Code
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://sites.google.com/view/centerpose" target="_blank" rel="noopener">
                                Project
                            </a>


                        </div>


                    </div>
                    <div class="ml-3">


                        <a href="/publication/centerpose/">
                            <img src="/publication/centerpose/icon_hu0b4de7f3e22a79ce2c44f67ccac8a930_5771698_200x0_resize_lanczos_2.png"
                                 alt="">
                        </a>

                    </div>
                </div>


                <div class="media stream-item">
                    <div class="media-body">

                        <h3 class="article-title mb-0 mt-0">
                            <a href="/publication/mvml/">Multi-view Fusion for Multi-level Robotic Scene
                                Understanding</a>
                        </h3>


                        <div class="article-style">


                            <p>A system for multi-level scene awareness for robotic manipulation, including three types
                                of information: 1) a point cloud representation of all the surfaces in the scene, for
                                the purpose of obstacle avoidance. 2) the rough pose of unknown objects from categories
                                corresponding to primitive shapes (e.g., cuboids and cylinders), and 3) full 6-DoF pose
                                of known objects.</p>
                        </div>


                        <div class="stream-meta article-metadata">


                            <div>


                                <span><strong>Yunzhi Lin</strong></span>, <span>Jonathan Tremblay</span>, <span>Stephen Tyree</span>,
                                <span>Patricio A. Vela</span>, <span>Stan Birchfield</span>

                            </div>

                        </div>


                        <div class="article-metadata-publication-short">IROS 2021</div>


                        <div class="btn-links">


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2103.13539"
                               target="_blank" rel="noopener">
                                Preprint
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="/pdf/Multi-view%20Fusion%20for%20Multi-level%20Robotic%20Scene%20Understanding.pdf"
                               target="_blank" rel="noopener">
                                PDF
                            </a>


                            <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                    data-filename="/publication/mvml/cite.bib">
                                Cite
                            </button>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://github.com/swtyree/hope-dataset" target="_blank" rel="noopener">
                                Dataset
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://research.nvidia.com/publication/2021-09_multi-view-fusion-multi-level-robotic-scene-understanding"
                               target="_blank" rel="noopener">
                                Project
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://youtu.be/FuqMxuODGlw"
                               target="_blank" rel="noopener">
                                Video
                            </a>


                        </div>


                    </div>
                    <div class="ml-3">


                        <a href="/publication/mvml/">
                            <img src="/publication/mvml/icon_huce03990db1b0844b0d0a6308db128f8c_1984179_200x0_resize_lanczos_2.png"
                                 alt="">
                        </a>

                    </div>
                </div>


                <div class="media stream-item">
                    <div class="media-body">

                        <h3 class="article-title mb-0 mt-0">
                            <a href="/publication/primitiveshapes/">Using Synthetic Data and Deep Networks to Recognize
                                Primitive Shapes for Object Grasping</a>
                        </h3>


                        <div class="article-style">


                            <p>A segmentation-based architecture proposed to decompose objects into multiple primitive
                                shapes from monocular depth input for robotic manipulation.</p>
                        </div>


                        <div class="stream-meta article-metadata">


                            <div>


                                <span><strong>Yunzhi Lin</strong></span>, <span>Chao Tang</span>, <span>Fujen Chu</span>,
                                <span>Patricio A. Vela</span>

                            </div>

                        </div>


                        <div class="article-metadata-publication-short">ICRA 2020</div>


                        <div class="btn-links">


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/1909.08508"
                               target="_blank" rel="noopener">
                                Preprint
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="/pdf/Using%20Synthetic%20Data%20and%20Deep%20Networks%20to%20Recognize%20Primitive%20Shapes%20for%20Object%20Grasping.pdf"
                               target="_blank" rel="noopener">
                                PDF
                            </a>


                            <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                    data-filename="/publication/primitiveshapes/cite.bib">
                                Cite
                            </button>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://github.com/ivalab/grasp_primitiveShape" target="_blank" rel="noopener">
                                Code
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://sites.google.com/view/primitive-shape-grasping" target="_blank"
                               rel="noopener">
                                Project
                            </a>


                            <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                               href="https://www.youtube.com/watch?v=AZuLpEzQMYQ" target="_blank" rel="noopener">
                                Video
                            </a>


                        </div>


                    </div>
                    <div class="ml-3">


                        <a href="/publication/primitiveshapes/">
                            <img src="/publication/primitiveshapes/icon_hu5fe2e4ad5dde86f9f294014c350c3ab0_27371_200x0_resize_q90_lanczos.jpg"
                                 alt="">
                        </a>

                    </div>
                </div>


            </div>
        </div>

    </div>
</section>


<section id="publications" class="home-section wg-pages   ">
    <div class="container">


        <div class="row">
            <div class="col-12 col-lg-4 section-heading">
                <h1>Publications</h1>

            </div>
            <div class="col-12 col-lg-8">

                <div class="alert alert-note">
                    <div>
                        Quickly discover relevant content by <a href="https://yunzhi.netlify.com/publication/"
                                                                target="_blank">filtering publications</a>.
                    </div>
                </div>


                <div class="view-list-item">
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <a href="/publication/inerfparallel/">Parallel Inversion of Neural Radiance Fields for Robust Pose
                        Estimation</a>


                    <div class="article-metadata">


                        <span><strong>Yunzhi Lin</strong></span>, <span>Thomas Müller</span>,
                        <span>Jonathan Tremblay</span>, <span>Bowen Wen</span>, <span>Stephen Tyree</span>, <span>Alex Evans</span>,
                        <span>Patricio A. Vela</span>, <span>Stan Birchfield</span>

                    </div>


                    <div class="article-metadata">IEEE International Conference on Robotics and Automation (ICRA 2023)
                    </div>


                    <div class="btn-links">


                        <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2210.10108"
                           target="_blank" rel="noopener">
                            Preprint
                        </a>


                        <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                data-filename="/publication/inerfparallel/cite.bib">
                            Cite
                        </button>


                        <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://pnerfp.github.io/"
                           target="_blank" rel="noopener">
                            Project
                        </a>


                    </div>


                </div>


                <div class="view-list-item">
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <a href="/publication/centerposetrack/">Keypoint-Based Category-Level Object Pose Tracking from an
                        RGB Sequence with Uncertainty Estimation</a>


                    <div class="article-metadata">


                        <span><strong>Yunzhi Lin</strong></span>, <span>Jonathan Tremblay</span>,
                        <span>Stephen Tyree</span>, <span>Patricio A. Vela</span>, <span>Stan Birchfield</span>

                    </div>


                    <div class="article-metadata">IEEE International Conference on Robotics and Automation (ICRA 2022)
                    </div>


                    <div class="btn-links">


                        <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2205.11047"
                           target="_blank" rel="noopener">
                            Preprint
                        </a>


                        <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                data-filename="/publication/centerposetrack/cite.bib">
                            Cite
                        </button>


                        <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/NVlabs/CenterPose"
                           target="_blank" rel="noopener">
                            Code
                        </a>


                        <a class="btn btn-outline-primary my-1 mr-1 btn-sm"
                           href="https://sites.google.com/view/centerposetrack" target="_blank" rel="noopener">
                            Project
                        </a>


                    </div>


                </div>


                <div class="view-list-item">
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <a href="/publication/sgl/">SGL: Symbolic Goal Learning in a Hybrid, Modular Framework for Human
                        Instruction Following</a>


                    <div class="article-metadata">


                        <span>Ruinian Xu</span>, <span>Hongyi Chen</span>, <span><strong>Yunzhi Lin</strong></span>,
                        <span>Patricio A. Vela</span>

                    </div>


                    <div class="article-metadata">IEEE Robotics and Automation Letters</div>


                    <div class="btn-links">


                        <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2202.12912"
                           target="_blank" rel="noopener">
                            Preprint
                        </a>


                        <button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
                                data-filename="/publication/sgl/cite.bib">
                            Cite
                        </button>


                    </div>


                </div>


                <div class="see-all">
                    <a href="/publication/">
                        See all publications
                        <i class="fas fa-angle-right"></i>
                    </a>
                </div>


            </div>
        </div>

    </div>
</section>

<section id="contact" class="home-section wg-contact   ">
    <div class="container">


        <div class="row contact-widget">
            <div class="col-12 col-lg-4 section-heading">
                <h1>Contact</h1>

            </div>
            <div class="col-12 col-lg-8">


                <ul class="fa-ul">


                    <li>
                        <i class="fa-li fas fa-envelope fa-2x" aria-hidden="true"></i>
                        <span id="person-email"><a href="mailto:yunzhi.lin@gatech.edu">yunzhi.lin@gatech.edu</a></span>
                    </li>


                    <li>
                        <i class="fa-li fas fa-phone fa-2x" aria-hidden="true"></i>
                        <span id="person-telephone"><a href="tel:%28470%29351-9500">(470)351-9500</a></span>
                    </li>


                    <li>
                        <i class="fa-li fas fa-map-marker fa-2x" aria-hidden="true"></i>
                        <span id="person-address">TSRB 440, 85 5th St NW, Atlanta, GA, 30308, United States</span>
                    </li>


                </ul>


            </div>
        </div>

    </div>
</section>

<script src="/js/mathjax-config.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js"
        integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js"
        integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"
        integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"
        integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full"
        integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>

<script>hljs.initHighlightingOnLoad();</script>


<script>
    const search_config = {"indexURI": "/index.json", "minLength": 1, "threshold": 0.3};
    const i18n = {"no_results": "No results found", "placeholder": "Search...", "results": "results found"};
    const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication': "Publications",
        'talk': "Talks"
    };
</script>

<script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
        <div class="search-hit-content">
            <div class="search-hit-name">
                <a href="{{relpermalink}}">{{title}}</a>
                <div class="article-metadata search-hit-type">{{type}}</div>
                <p class="search-hit-description">{{snippet}}</p>
            </div>
        </div>
    </div>
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js"
        integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js"
        integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>

<script src="/js/academic.min.6cbefe3f0755301b86cd38317ded9f54.js"></script>
<div class="container">
    <footer class="site-footer">


        <p class="powered-by">
            Yunzhi Lin &copy; 2022 &middot;

            Powered by the
            <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
            <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.


            <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>

        </p>
    </footer>

</div>
<div id="modal" class="modal fade" role="dialog">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title">Cite</h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true">&times;</span>
                </button>
            </div>
            <div class="modal-body">
                <pre><code class="tex hljs"></code></pre>
            </div>
            <div class="modal-footer">
                <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
                    <i class="fas fa-copy"></i> Copy
                </a>
                <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
                    <i class="fas fa-download"></i> Download
                </a>
                <div id="modal-error"></div>
            </div>
        </div>
    </div>
</div>

</body>
</html>
